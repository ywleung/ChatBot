{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_train_path = 'data/cornell/questions_train.txt'\n",
    "answers_train_path = 'data/cornell/answers_train.txt'\n",
    "questions_test_path = 'data/cornell/questions_test.txt'\n",
    "answers_test_path = 'data/cornell/answers_test.txt'\n",
    "\n",
    "questions_train, answers_train, questions_test, answers_test = [], [], [], []\n",
    "\n",
    "dataset_list = [questions_train, answers_train, questions_test, answers_test]\n",
    "path_list = [questions_train_path, answers_train_path, questions_test_path, answers_test_path]\n",
    "\n",
    "for dataset, path in zip(dataset_list, path_list):\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            dataset.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GloVe\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# convert GloVe vectors into the word2vec\n",
    "glove_file = 'glove.6B.50d.txt'\n",
    "tmp_file = 'glove_50d_word2vec.txt'\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "embeddings = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol = '<S>'\n",
    "end_symbol = '</S>'\n",
    "padding_symbol = '<PAD>'\n",
    "unknown_symbol = '<UNK>'\n",
    "\n",
    "special_symbols = [start_symbol, end_symbol, padding_symbol, unknown_symbol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):               \n",
    "    text = re.sub(r\"<[^>]*>\", \"\", text)\n",
    "    text = re.sub(r\"[<>]\", \"\", text)\n",
    "                       \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    preprocessed_dataset = []\n",
    "    for sentence in dataset:\n",
    "        cleaned_sentence = clean_text(sentence)\n",
    "        tokenized_sentence = nltk.word_tokenize(cleaned_sentence)\n",
    "        final_sentence = [word.lower() for word in tokenized_sentence]\n",
    "        preprocessed_dataset.append(final_sentence)\n",
    "        \n",
    "    return preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "tokenized_questions_train = preprocess_dataset(questions_train)\n",
    "tokenized_answers_train = preprocess_dataset(answers_train)\n",
    "\n",
    "# test set\n",
    "tokenized_questions_test = preprocess_dataset(questions_test)\n",
    "tokenized_answers_test = preprocess_dataset(answers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check occurances of words\n",
    "vocab_occ = {}\n",
    "for dataset in [tokenized_questions_train, tokenized_answers_train]:\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            vocab_occ[word] = vocab_occ.get(word, 0) + 1\n",
    "vocab_occ = sorted(vocab_occ.items(), key=lambda kv: kv[1])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(tokenized_questions, tokenized_answers, special_symbols):\n",
    "    word2id = {}\n",
    "    id2word = []\n",
    "    \n",
    "    for special_symbol in special_symbols:\n",
    "        id2word.append(special_symbol)\n",
    "        word2id[special_symbol] = id2word.index(special_symbol)\n",
    "        \n",
    "    vocab_set = set(word for dataset in [tokenized_questions, tokenized_answers]\n",
    "                    for sentence in dataset\n",
    "                    for word in sentence\n",
    "                    if word not in special_symbols)\n",
    "     \n",
    "    for word in vocab_set:\n",
    "        id2word.append(word)\n",
    "        word2id[word] = id2word.index(word)\n",
    "        \n",
    "    return word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id, id2word = build_dict(tokenized_questions_train, tokenized_answers_train, special_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_unk(dataset, word2id):\n",
    "    replaced_dataset = []\n",
    "    for sentence in dataset:\n",
    "        for i, word in enumerate(sentence):\n",
    "            if word not in word2id.keys():\n",
    "                sentence[i] = '<UNK>'\n",
    "        replaced_dataset.append(sentence)\n",
    "        \n",
    "    return replaced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_questions_test = replace_with_unk(tokenized_questions_test, word2id)\n",
    "tokenized_answers_test = replace_with_unk(tokenized_answers_test, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check oov\n",
    "oov = []\n",
    "for dataset in [tokenized_questions_test, tokenized_answers_test]:\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            if word not in word2id.keys():\n",
    "                oov.append(word)\n",
    "set(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1946"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of <UNK> in test set\n",
    "count = 0\n",
    "for dataset in [tokenized_questions_test, tokenized_answers_test]:\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            if word == '<UNK>':\n",
    "                count += 1\n",
    "                \n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings(word2id, embeddings, dim=300):\n",
    "    vocab_size = len(word2id)\n",
    "    embedding_matrix = np.random.normal(0, 1, (vocab_size, dim))\n",
    "    \n",
    "    for word, i in word2id.items():\n",
    "        try:\n",
    "            embedding_vector = embeddings.get_vector(word)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "customized_embeddings = build_embeddings(word2id, embeddings, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save customized_embeddings\n",
    "# path = 'word_embeddings_50d.txt'\n",
    "\n",
    "# np.savetxt(path, customized_embeddings, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word2id dictionary\n",
    "# path = 'word2id.txt'\n",
    "\n",
    "# out = open(path, 'w')\n",
    "# for word, i in word2id.items():\n",
    "#     print(word, i, sep=' ', file=out)\n",
    "# out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_ids(tokenized_sentence, word2id, padded_len):\n",
    "    num_pad = max(0, padded_len - 1 - len(tokenized_sentence))\n",
    "    sent = tokenized_sentence[:padded_len-1] + ['</S>']\n",
    "    sent = sent + ['<PAD>']*num_pad\n",
    "    sent_ids = [word2id[word] for word in sent]\n",
    "    \n",
    "    sent_len = min(len(tokenized_sentence)+1, padded_len)\n",
    "    \n",
    "    return sent_ids, sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_sentence(ids, id2word):\n",
    "    return [id2word[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_ids(sentences, word2id, max_len):\n",
    "    max_len_in_batch = min(max(len(s) for s in sentences) + 1, max_len)\n",
    "    batch_ids, batch_ids_len = [], []\n",
    "    for sentence in sentences:\n",
    "        ids, ids_len = sentence_to_ids(sentence, word2id, max_len_in_batch)\n",
    "        batch_ids.append(ids)\n",
    "        batch_ids_len.append(ids_len)\n",
    "        \n",
    "    return batch_ids, batch_ids_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(samples, batch_size=32):\n",
    "    X, Y = [], []\n",
    "    for i, (x, y) in enumerate(samples, 1):\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        if i % batch_size == 0:\n",
    "            yield X, Y\n",
    "            X, Y = [], []\n",
    "    if X and Y:\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_placeholders(self):\n",
    "    # placeholders for input and its actual lengths\n",
    "    self.input_batch = tf.placeholder(shape=(None, None), dtype=tf.int32, name='input_batch')\n",
    "    self.input_batch_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='input_batch_lengths')\n",
    "    \n",
    "    # placeholders for groundtruth and its actual lenghts\n",
    "    self.ground_truth = tf.placeholder(shape=(None, None), dtype=tf.int32, name='ground_truth')\n",
    "    self.ground_truth_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='ground_truth_lengths')\n",
    "    \n",
    "    # placeholders for dropout_rate and learning_rate\n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(self, embeddings_matrix):\n",
    "    self.embeddings = tf.get_variable(name='embeddings', \n",
    "                                     shape=embeddings_matrix.shape,\n",
    "                                     initializer=tf.constant_initializer(embeddings_matrix),\n",
    "                                     trainable=False)\n",
    "    self.input_batch_embedded = tf.nn.embedding_lookup(self.embeddings, self.input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__create_embeddings = classmethod(create_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(self, hidden_size):\n",
    "    forward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.nn.rnn_cell.BasicLSTMCell(num_units=hidden_size),\n",
    "        input_keep_prob=self.dropout_ph,\n",
    "        output_keep_prob=self.dropout_ph,\n",
    "        state_keep_prob=self.dropout_ph)\n",
    "    \n",
    "    backward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.nn.rnn_cell.BasicLSTMCell(num_units=hidden_size),\n",
    "        input_keep_prob=self.dropout_ph,\n",
    "        output_keep_prob=self.dropout_ph,\n",
    "        state_keep_prob=self.dropout_ph)\n",
    "    \n",
    "    output, final_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=forward_cell,\n",
    "        cell_bw=backward_cell,\n",
    "        inputs=self.input_batch_embedded,\n",
    "        sequence_length=self.input_batch_lengths,\n",
    "        dtype=tf.float32)\n",
    "    \n",
    "    self.encoder_output = tf.concat([output[0], output[1]], axis=2)\n",
    "    \n",
    "    encoder_final_state_c = tf.concat([final_state[0].c, final_state[1].c], axis=1)\n",
    "    encoder_final_state_h = tf.concat([final_state[0].h, final_state[1].h], axis=1)\n",
    "    self.encoder_final_state = tf.contrib.rnn.LSTMStateTuple(c=encoder_final_state_c, h=encoder_final_state_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__build_encoder = classmethod(build_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder(self, hidden_size, vocab_size, max_iter, start_symbol_id, end_symbol_id):\n",
    "    batch_size = tf.shape(self.input_batch)[0]\n",
    "    start_tokens = tf.fill([batch_size], start_symbol_id)\n",
    "    ground_truth_as_input = tf.concat([tf.expand_dims(start_tokens, 1), self.ground_truth], 1)\n",
    "    \n",
    "    # Use the embedding layer defined before to lookup embedings for ground_truth_as_input\n",
    "    self.ground_truth_embedded = tf.nn.embedding_lookup(self.embeddings, ground_truth_as_input)\n",
    "    \n",
    "    # Create TrainingHelper for the train stage\n",
    "    train_helper = tf.contrib.seq2seq.TrainingHelper(self.ground_truth_embedded,\n",
    "                                                     self.ground_truth_lengths)\n",
    "        \n",
    "    # Create GreedyEmbeddingHelper for the inference stage\n",
    "    infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embeddings, start_tokens, end_symbol_id)\n",
    "    \n",
    "    def decode(helper, scope, reuse=None):\n",
    "        \"\"\"Creates decoder and return the results of the decoding with a given helper.\"\"\"\n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                num_units=hidden_size, \n",
    "                memory=self.encoder_output,\n",
    "                memory_sequence_length=self.input_batch_lengths)\n",
    "            \n",
    "            cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=hidden_size*2, reuse=reuse)\n",
    " \n",
    "            attention_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell, attention_mechanism, attention_layer_size=hidden_size)\n",
    "            \n",
    "            decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "                attention_cell, vocab_size, reuse=reuse)\n",
    "            \n",
    "            decoder_initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell=decoder_cell,\n",
    "                helper=helper,\n",
    "                initial_state=decoder_initial_state)\n",
    "            \n",
    "            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder=decoder,\n",
    "                maximum_iterations=max_iter,\n",
    "                output_time_major=False,\n",
    "                impute_finished=True)\n",
    "            \n",
    "            return outputs\n",
    "    \n",
    "    self.train_outputs = decode(train_helper, 'decode')\n",
    "    self.infer_outputs = decode(infer_helper, 'decode', reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__build_decoder = classmethod(build_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self):\n",
    "    \"\"\"Computes sequence loss (masked cross-entopy loss with logits).\"\"\"\n",
    "    \n",
    "    weights = tf.cast(tf.sequence_mask(self.ground_truth_lengths), dtype=tf.float32)\n",
    "    \n",
    "    self.loss = tf.contrib.seq2seq.sequence_loss(self.train_outputs.rnn_output,\n",
    "                                                 self.ground_truth,\n",
    "                                                 weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_optimization(self):\n",
    "    self.train_op = tf.contrib.layers.optimize_loss(loss=self.loss,\n",
    "                                                    optimizer='Adam',\n",
    "                                                    learning_rate=self.learning_rate_ph,\n",
    "                                                    clip_gradients=1.0,\n",
    "                                                    global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(self, embeddings_matrix, hidden_size, vocab_size, max_iter, \n",
    "               start_symbol_id, end_symbol_id, padding_symbol_id):\n",
    "    self.__declare_placeholders()\n",
    "    self.__create_embeddings(embeddings_matrix)\n",
    "    self.__build_encoder(hidden_size)\n",
    "    self.__build_decoder(hidden_size, vocab_size, max_iter, start_symbol_id, end_symbol_id)\n",
    "    \n",
    "    self.__compute_loss()\n",
    "    self.__perform_optimization()\n",
    "    \n",
    "    self.train_predictions = self.train_outputs.sample_id\n",
    "    self.infer_predictions = self.infer_outputs.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(self, session, X, X_seq_len, Y, Y_seq_len, learning_rate, dropout_keep_probability):\n",
    "    feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: X_seq_len,\n",
    "            self.ground_truth: Y,\n",
    "            self.ground_truth_lengths: Y_seq_len,\n",
    "            self.learning_rate_ph: learning_rate,\n",
    "            self.dropout_ph: dropout_keep_probability\n",
    "        }\n",
    "    pred, loss, _ = session.run([\n",
    "            self.train_predictions,\n",
    "            self.loss,\n",
    "            self.train_op], feed_dict=feed_dict)\n",
    "    return pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_batch(self, session, X, X_seq_len):\n",
    "    feed_dict = {self.input_batch: X, self.input_batch_lengths: X_seq_len}\n",
    "    pred = session.run([\n",
    "            self.infer_predictions\n",
    "        ], feed_dict=feed_dict)[0]\n",
    "    return pred\n",
    "\n",
    "def predict_for_batch_with_loss(self, session, X, X_seq_len, Y, Y_seq_len):\n",
    "    feed_dict = {self.input_batch: X, \n",
    "                 self.input_batch_lengths: X_seq_len,\n",
    "                 self.ground_truth: Y,\n",
    "                 self.ground_truth_lengths: Y_seq_len}\n",
    "    pred, loss = session.run([\n",
    "            self.infer_predictions,\n",
    "            self.loss,\n",
    "        ], feed_dict=feed_dict)\n",
    "    return pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2SeqModel.predict_for_batch = classmethod(predict_for_batch)\n",
    "Seq2SeqModel.predict_for_batch_with_loss = classmethod(predict_for_batch_with_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    embeddings_matrix=customized_embeddings,\n",
    "    hidden_size=128,\n",
    "    vocab_size=customized_embeddings.shape[0],\n",
    "    max_iter=10, \n",
    "    start_symbol_id=word2id['<S>'],\n",
    "    end_symbol_id=word2id['</S>'],\n",
    "    padding_symbol_id=word2id['<PAD>'])\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "learning_rate = 0.001\n",
    "dropout_keep_probability = 0.5\n",
    "max_len = 10\n",
    "learning_rate_decay = 0.75\n",
    "min_learning_rate = 0.0001\n",
    "\n",
    "n_step = int(len(questions_train)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train: epoch 1\n",
      "Epoch:   1/10, Step:    1/6232, Loss: 10.956, Seconds: 2.41\n",
      "Epoch:   1/10, Step:  201/6232, Loss:  5.178, Seconds: 44.51\n",
      "Epoch:   1/10, Step:  401/6232, Loss:  5.197, Seconds: 86.58\n",
      "Epoch:   1/10, Step:  601/6232, Loss:  4.904, Seconds: 128.75\n",
      "Epoch:   1/10, Step:  801/6232, Loss:  5.154, Seconds: 170.86\n",
      "Epoch:   1/10, Step: 1001/6232, Loss:  4.590, Seconds: 212.93\n",
      "Epoch:   1/10, Step: 1201/6232, Loss:  4.924, Seconds: 255.05\n",
      "Epoch:   1/10, Step: 1401/6232, Loss:  4.818, Seconds: 297.14\n",
      "Epoch:   1/10, Step: 1601/6232, Loss:  4.789, Seconds: 339.40\n",
      "Epoch:   1/10, Step: 1801/6232, Loss:  4.689, Seconds: 381.54\n",
      "Epoch:   1/10, Step: 2001/6232, Loss:  4.490, Seconds: 423.69\n",
      "Epoch:   1/10, Step: 2201/6232, Loss:  4.254, Seconds: 465.80\n",
      "Epoch:   1/10, Step: 2401/6232, Loss:  4.188, Seconds: 507.91\n",
      "Epoch:   1/10, Step: 2601/6232, Loss:  4.323, Seconds: 550.02\n",
      "Epoch:   1/10, Step: 2801/6232, Loss:  4.656, Seconds: 592.25\n",
      "Epoch:   1/10, Step: 3001/6232, Loss:  4.620, Seconds: 634.47\n",
      "Epoch:   1/10, Step: 3201/6232, Loss:  4.380, Seconds: 676.50\n",
      "Epoch:   1/10, Step: 3401/6232, Loss:  4.271, Seconds: 718.42\n",
      "Epoch:   1/10, Step: 3601/6232, Loss:  4.224, Seconds: 760.28\n",
      "Epoch:   1/10, Step: 3801/6232, Loss:  4.401, Seconds: 802.46\n",
      "Epoch:   1/10, Step: 4001/6232, Loss:  4.257, Seconds: 844.61\n",
      "Epoch:   1/10, Step: 4201/6232, Loss:  4.534, Seconds: 886.89\n",
      "Epoch:   1/10, Step: 4401/6232, Loss:  4.377, Seconds: 928.86\n",
      "Epoch:   1/10, Step: 4601/6232, Loss:  4.392, Seconds: 971.02\n",
      "Epoch:   1/10, Step: 4801/6232, Loss:  4.490, Seconds: 1013.69\n",
      "Epoch:   1/10, Step: 5001/6232, Loss:  3.967, Seconds: 1056.12\n",
      "Epoch:   1/10, Step: 5201/6232, Loss:  4.200, Seconds: 1098.58\n",
      "Epoch:   1/10, Step: 5401/6232, Loss:  4.173, Seconds: 1141.05\n",
      "Epoch:   1/10, Step: 5601/6232, Loss:  3.877, Seconds: 1183.32\n",
      "Epoch:   1/10, Step: 5801/6232, Loss:  4.141, Seconds: 1225.78\n",
      "Epoch:   1/10, Step: 6001/6232, Loss:  4.089, Seconds: 1268.04\n",
      "Epoch:   1/10, Step: 6201/6232, Loss:  4.441, Seconds: 1310.10\n",
      "\n",
      "Test: epoch 1 loss 4.2318664 Second: 89.52899169921875\n",
      "X: that 's alright , we got a pressing engagement </S>\n",
      "Y: yeah , right outside the door ! </S> <PAD> <PAD>\n",
      "O: i 'm not sure . </S>\n",
      "\n",
      "X: de gaulle 's gon na be there . and </S>\n",
      "Y: then call him . i 'm sure it was </S>\n",
      "O: i 'm sorry . </S> <S>\n",
      "\n",
      "X: my lips may give a message better of christmas </S>\n",
      "Y: aye . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "O: i 'm not sure . </S>\n",
      "\n",
      "No Improvement\n",
      "------------------------------\n",
      "Train: epoch 2\n",
      "Epoch:   2/10, Step:    1/6232, Loss:  4.304, Seconds: 0.22\n",
      "Epoch:   2/10, Step:  201/6232, Loss:  4.069, Seconds: 42.44\n",
      "Epoch:   2/10, Step:  401/6232, Loss:  4.143, Seconds: 84.70\n",
      "Epoch:   2/10, Step:  601/6232, Loss:  4.519, Seconds: 126.93\n",
      "Epoch:   2/10, Step:  801/6232, Loss:  3.941, Seconds: 169.17\n",
      "Epoch:   2/10, Step: 1001/6232, Loss:  4.308, Seconds: 211.56\n",
      "Epoch:   2/10, Step: 1201/6232, Loss:  3.702, Seconds: 253.57\n",
      "Epoch:   2/10, Step: 1401/6232, Loss:  4.221, Seconds: 295.64\n",
      "Epoch:   2/10, Step: 1601/6232, Loss:  4.128, Seconds: 337.47\n",
      "Epoch:   2/10, Step: 1801/6232, Loss:  3.978, Seconds: 379.29\n",
      "Epoch:   2/10, Step: 2001/6232, Loss:  3.824, Seconds: 421.45\n",
      "Epoch:   2/10, Step: 2201/6232, Loss:  4.302, Seconds: 463.63\n",
      "Epoch:   2/10, Step: 2401/6232, Loss:  4.014, Seconds: 505.64\n",
      "Epoch:   2/10, Step: 2601/6232, Loss:  4.048, Seconds: 547.50\n",
      "Epoch:   2/10, Step: 2801/6232, Loss:  4.104, Seconds: 589.30\n",
      "Epoch:   2/10, Step: 3001/6232, Loss:  3.553, Seconds: 631.23\n",
      "Epoch:   2/10, Step: 3201/6232, Loss:  4.169, Seconds: 673.03\n",
      "Epoch:   2/10, Step: 3401/6232, Loss:  3.704, Seconds: 714.87\n",
      "Epoch:   2/10, Step: 3601/6232, Loss:  4.262, Seconds: 756.65\n",
      "Epoch:   2/10, Step: 3801/6232, Loss:  3.979, Seconds: 798.44\n",
      "Epoch:   2/10, Step: 4001/6232, Loss:  3.938, Seconds: 840.24\n",
      "Epoch:   2/10, Step: 4201/6232, Loss:  3.946, Seconds: 882.13\n",
      "Epoch:   2/10, Step: 4401/6232, Loss:  4.189, Seconds: 923.95\n",
      "Epoch:   2/10, Step: 4601/6232, Loss:  4.125, Seconds: 965.80\n",
      "Epoch:   2/10, Step: 4801/6232, Loss:  4.106, Seconds: 1007.66\n",
      "Epoch:   2/10, Step: 5001/6232, Loss:  3.917, Seconds: 1049.53\n",
      "Epoch:   2/10, Step: 5201/6232, Loss:  3.831, Seconds: 1091.40\n",
      "Epoch:   2/10, Step: 5401/6232, Loss:  4.028, Seconds: 1133.36\n",
      "Epoch:   2/10, Step: 5601/6232, Loss:  4.134, Seconds: 1175.25\n",
      "Epoch:   2/10, Step: 5801/6232, Loss:  3.791, Seconds: 1217.05\n",
      "Epoch:   2/10, Step: 6001/6232, Loss:  4.135, Seconds: 1258.83\n",
      "Epoch:   2/10, Step: 6201/6232, Loss:  3.862, Seconds: 1300.59\n",
      "\n",
      "Test: epoch 2 loss 4.11336 Second: 112.35866403579712\n",
      "X: damn . 'and wainwright ... ' </S> <PAD> <PAD> <PAD>\n",
      "Y: cartwright . </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "O: i 'm not sure . </S> <S> <S> <S> <S>\n",
      "\n",
      "X: can the professor repair his own mind ? </S> <PAD>\n",
      "Y: i 'm afraid without cerebro 's help , he </S>\n",
      "O: i do n't know . </S> <S> <S> <S> <S>\n",
      "\n",
      "X: three bullets ! any good ? </S> <PAD> <PAD> <PAD>\n",
      "Y: beats king up . </S> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "O: i 'm not sure . </S> <S> <S> <S> <S>\n",
      "\n",
      "No Improvement\n",
      "------------------------------\n",
      "Train: epoch 3\n",
      "Epoch:   3/10, Step:    1/6232, Loss:  3.914, Seconds: 0.22\n",
      "Epoch:   3/10, Step:  201/6232, Loss:  3.633, Seconds: 41.96\n",
      "Epoch:   3/10, Step:  401/6232, Loss:  3.597, Seconds: 83.76\n",
      "Epoch:   3/10, Step:  601/6232, Loss:  3.981, Seconds: 125.58\n",
      "Epoch:   3/10, Step:  801/6232, Loss:  3.812, Seconds: 167.39\n",
      "Epoch:   3/10, Step: 1001/6232, Loss:  3.951, Seconds: 209.17\n",
      "Epoch:   3/10, Step: 1201/6232, Loss:  4.269, Seconds: 250.97\n",
      "Epoch:   3/10, Step: 1401/6232, Loss:  3.949, Seconds: 292.78\n",
      "Epoch:   3/10, Step: 1601/6232, Loss:  3.918, Seconds: 334.61\n",
      "Epoch:   3/10, Step: 1801/6232, Loss:  4.040, Seconds: 376.44\n",
      "Epoch:   3/10, Step: 2001/6232, Loss:  3.779, Seconds: 418.24\n",
      "Epoch:   3/10, Step: 2201/6232, Loss:  4.190, Seconds: 460.08\n",
      "Epoch:   3/10, Step: 2401/6232, Loss:  3.668, Seconds: 501.90\n",
      "Epoch:   3/10, Step: 2601/6232, Loss:  3.866, Seconds: 543.74\n",
      "Epoch:   3/10, Step: 2801/6232, Loss:  3.658, Seconds: 585.56\n",
      "Epoch:   3/10, Step: 3001/6232, Loss:  4.030, Seconds: 627.39\n",
      "Epoch:   3/10, Step: 3201/6232, Loss:  3.958, Seconds: 669.23\n",
      "Epoch:   3/10, Step: 3401/6232, Loss:  4.299, Seconds: 711.01\n",
      "Epoch:   3/10, Step: 3601/6232, Loss:  4.072, Seconds: 752.85\n",
      "Epoch:   3/10, Step: 3801/6232, Loss:  4.325, Seconds: 794.67\n",
      "Epoch:   3/10, Step: 4001/6232, Loss:  3.608, Seconds: 836.51\n",
      "Epoch:   3/10, Step: 4201/6232, Loss:  3.833, Seconds: 878.34\n",
      "Epoch:   3/10, Step: 4401/6232, Loss:  4.245, Seconds: 920.14\n",
      "Epoch:   3/10, Step: 4601/6232, Loss:  3.885, Seconds: 961.95\n",
      "Epoch:   3/10, Step: 4801/6232, Loss:  3.724, Seconds: 1003.73\n",
      "Epoch:   3/10, Step: 5001/6232, Loss:  3.976, Seconds: 1045.34\n",
      "Epoch:   3/10, Step: 5201/6232, Loss:  3.780, Seconds: 1086.88\n",
      "Epoch:   3/10, Step: 5401/6232, Loss:  3.760, Seconds: 1128.40\n",
      "Epoch:   3/10, Step: 5601/6232, Loss:  3.861, Seconds: 1169.96\n",
      "Epoch:   3/10, Step: 5801/6232, Loss:  3.961, Seconds: 1211.49\n",
      "Epoch:   3/10, Step: 6001/6232, Loss:  3.629, Seconds: 1253.01\n",
      "Epoch:   3/10, Step: 6201/6232, Loss:  3.845, Seconds: 1294.53\n",
      "\n",
      "Test: epoch 3 loss 4.0612864 Second: 112.4085066318512\n",
      "X: when they 've got your recruiting technique ? not </S>\n",
      "Y: oh . <UNK> is n't exactly by the book </S>\n",
      "O: i do n't know . </S> <S> <S> <S> <S>\n",
      "\n",
      "X: i do n't drink . </S> <PAD> <PAD> <PAD> <PAD>\n",
      "Y: it 's for your finger . </S> <PAD> <PAD> <PAD>\n",
      "O: you 're not going to be a little bit </S>\n",
      "\n",
      "X: h-how do i get ... ? </S> <PAD> <PAD> <PAD>\n",
      "Y: way we came . take the first tunnel to </S>\n",
      "O: i 'm not going to be a little bit </S>\n",
      "\n",
      "No Improvement\n",
      "------------------------------\n",
      "Train: epoch 4\n",
      "Epoch:   4/10, Step:    1/6232, Loss:  3.492, Seconds: 0.22\n",
      "Epoch:   4/10, Step:  201/6232, Loss:  3.720, Seconds: 41.78\n",
      "Epoch:   4/10, Step:  401/6232, Loss:  4.038, Seconds: 83.43\n",
      "Epoch:   4/10, Step:  601/6232, Loss:  3.948, Seconds: 125.05\n",
      "Epoch:   4/10, Step:  801/6232, Loss:  3.900, Seconds: 166.62\n",
      "Epoch:   4/10, Step: 1001/6232, Loss:  3.648, Seconds: 208.18\n",
      "Epoch:   4/10, Step: 1201/6232, Loss:  3.690, Seconds: 249.74\n",
      "Epoch:   4/10, Step: 1401/6232, Loss:  3.864, Seconds: 291.34\n",
      "Epoch:   4/10, Step: 1601/6232, Loss:  3.710, Seconds: 332.93\n",
      "Epoch:   4/10, Step: 1801/6232, Loss:  3.940, Seconds: 374.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   4/10, Step: 2001/6232, Loss:  3.623, Seconds: 416.15\n",
      "Epoch:   4/10, Step: 2201/6232, Loss:  3.651, Seconds: 457.73\n",
      "Epoch:   4/10, Step: 2401/6232, Loss:  3.455, Seconds: 499.31\n",
      "Epoch:   4/10, Step: 2601/6232, Loss:  3.724, Seconds: 540.91\n",
      "Epoch:   4/10, Step: 2801/6232, Loss:  3.941, Seconds: 582.49\n",
      "Epoch:   4/10, Step: 3001/6232, Loss:  3.731, Seconds: 624.03\n",
      "Epoch:   4/10, Step: 3201/6232, Loss:  3.774, Seconds: 665.60\n",
      "Epoch:   4/10, Step: 3401/6232, Loss:  3.993, Seconds: 707.18\n",
      "Epoch:   4/10, Step: 3601/6232, Loss:  4.091, Seconds: 748.74\n",
      "Epoch:   4/10, Step: 3801/6232, Loss:  4.154, Seconds: 790.36\n",
      "Epoch:   4/10, Step: 4001/6232, Loss:  3.635, Seconds: 831.95\n",
      "Epoch:   4/10, Step: 4201/6232, Loss:  3.897, Seconds: 873.51\n",
      "Epoch:   4/10, Step: 4401/6232, Loss:  3.818, Seconds: 915.09\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "all_model_predictions = []\n",
    "all_ground_truth = []\n",
    "\n",
    "display_iter = 200\n",
    "checkpoint = \"model/50d/best_model.ckpt\"\n",
    "stop_early = 0\n",
    "stop = 5\n",
    "# validation_check = ((len(tokenized_questions_train))//batch_size//2)-1\n",
    "summary_test_loss = []\n",
    "\n",
    "train_set = list(zip(tokenized_questions_train, tokenized_answers_train))\n",
    "test_set = list(zip(tokenized_questions_test, tokenized_answers_test))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "       \n",
    "    print('-'*30)\n",
    "    print('Train: epoch', epoch + 1)\n",
    "    total_train_time = 0\n",
    "    for n_iter, (X_batch, Y_batch) in enumerate(generate_batches(train_set, batch_size)):\n",
    "        start_time = time.time()      \n",
    "        X_ids, X_sent_lens = batch_to_ids(X_batch, word2id, max_len)\n",
    "        Y_ids, Y_sent_lens = batch_to_ids(Y_batch, word2id, max_len)\n",
    "        \n",
    "        predictions, loss = model.train_on_batch(\n",
    "            session,\n",
    "            X_ids,\n",
    "            X_sent_lens,\n",
    "            Y_ids,\n",
    "            Y_sent_lens,\n",
    "            learning_rate,\n",
    "            dropout_keep_probability)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - start_time\n",
    "        total_train_time += batch_time\n",
    "        if n_iter % display_iter == 0:\n",
    "            print(\"Epoch: {:>3}/{}, Step: {:>4}/{}, Loss: {:>6.3f}, Seconds: {:>4.2f}\"\n",
    "                  .format(epoch+1, n_epochs, n_iter+1, n_step, loss, total_train_time))\n",
    "#             print(\"Epoch: [%d/%d], step: [%d/%d], loss: %f\" % (epoch+1, n_epochs, n_iter+1, n_step, loss))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    epoch_test_loss = []\n",
    "    for n_iter, (X_batch, Y_batch) in enumerate(generate_batches(test_set, batch_size)):        \n",
    "        X, X_sent_lens = batch_to_ids(X_batch, word2id, max_len)\n",
    "        Y, Y_sent_lens = batch_to_ids(Y_batch, word2id, max_len)\n",
    "\n",
    "        predictions, loss = model.predict_for_batch_with_loss(\n",
    "            session,\n",
    "            X,\n",
    "            X_sent_lens,\n",
    "            Y,\n",
    "            Y_sent_lens)\n",
    "        \n",
    "        epoch_test_loss.append(loss)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    batch_time = end_time - start_time\n",
    "    print('')\n",
    "    print('Test: epoch', epoch+1, 'loss', np.mean(epoch_test_loss), 'Second:', batch_time)   \n",
    "    for x, y, p in list(zip(X, Y, predictions))[:3]:\n",
    "        print('X:', ' '.join(ids_to_sentence(x, id2word)))\n",
    "        print('Y:', ' '.join(ids_to_sentence(y, id2word)))\n",
    "        print('O:', ' '.join(ids_to_sentence(p, id2word)))\n",
    "        print('')\n",
    "    \n",
    "    # reduce learning rate\n",
    "    learning_rate *= learning_rate_decay\n",
    "    learning_rate = max(learning_rate, min_learning_rate)\n",
    "    \n",
    "    summary_test_loss.append(np.mean(epoch_test_loss))\n",
    "    if np.mean(epoch_test_loss) <= min(summary_test_loss):\n",
    "        print('New Record!')\n",
    "        print('')\n",
    "        stop_early = 0\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(session, checkpoint)\n",
    "    else:\n",
    "        print('No Improvement')\n",
    "        stop_early += 1\n",
    "        if stop_early == stop:\n",
    "            break\n",
    "            \n",
    "print('\\n...training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
